{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from sacred import Experiment\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import Datasets\n",
    "from Input import Input as Input\n",
    "from Input import batchgenerators as batchgen\n",
    "import Models.WGAN_Critic\n",
    "import Models.Unet\n",
    "import Utils\n",
    "import cPickle as pickle\n",
    "import Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/051 - AM Contra - Heart Peripheral/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/052 - ANiMAL - Easy Tiger/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/053 - Actions - Devil's Words/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/054 - Actions - South Of The Water/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/055 - Angels In Amplifiers - I'm Alright/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/056 - Arise - Run Run Run/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/057 - BKS - Bulldozer/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/058 - Ben Carrigan - We'll Talk About It All Tonight/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/094 - Titanium - Haunted Age/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/095 - Traffic Experiment - Once More (With Feeling)/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/096 - Triviul - Angelsaint/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/097 - Triviul feat. The Fiend - Widow/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/098 - Wall Of Death - Femme/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/099 - Young Griffo - Blood To Bone/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/100 - Young Griffo - Pennies/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/084 - Secretariat - Borderline/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/085 - Side Effects Project - Sing With Me/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/086 - Skelpolu - Human Mistakes/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/087 - Skelpolu - Together Alone/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/088 - Speak Softly - Like Horses/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/089 - St Vitus - Word Gets Around/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/090 - The Doppler Shift - Atrophy/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/091 - The Long Wait - Dark Horses/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/092 - The Sunshine Garcia Band - For I Am The Moon/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/076 - Little Chicago's Finest - My Own/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/077 - Lyndsey Ollard - Catching Up/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/078 - Moosmusic - Big Dummy Shake/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/079 - Mu - Too Bright/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/080 - North To Alaska - All The Same/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/081 - Patrick Talbot - Set Me Free/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/082 - Punkdisco - Oral Hygiene/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/068 - Giselle - Moss/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/069 - Hollow Ground - Left Blind/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/070 - James May - All Souls Moon/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/071 - James May - If You Say/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/072 - Jay Menon - Through My Eyes/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/073 - Johnny Lokke - Whisper To A Scream/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/074 - Juliet's Rescue - Heartbeats/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/059 - Black Bloc - If You Want Success/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/067 - Georgia Wonder - Siren/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/075 - Leaf - Summerghost/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/083 - Remember December - C U Next Time/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/093 - Tim Taler - Stalker/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/060 - Buitraker - Revo X/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/061 - Chris Durban - Celebrate/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/062 - Cristina Vane - So Easy/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/063 - Detsky Sad - Walkie Talkie/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/064 - Enda Reilly - Cur An Long Ag Seol/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/065 - Fergessen - Nos Palpitants/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Dev/066 - Flags - 54/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/001 - ANiMAL - Clinic A/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/002 - ANiMAL - Rockshow/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/003 - Actions - One Minute Smile/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/004 - Al James - Schoolboy Facination/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/005 - Angela Thomas Wade - Milk Cow Blues/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/006 - Atlantis Bound - It Was My Fault For Waiting/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/007 - BKS - Too Much/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/008 - Bill Chudziak - Children Of No-one/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/009 - Bobby Nobody - Stitch Up/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/010 - Carlos Gonzalez - A Place For Us/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/011 - Cnoc An Tursa - Bannockburn/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/012 - Dark Ride - Burning Bridges/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/040 - The Long Wait - Back Home To Blue/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/041 - The Mountaineering Club - Mallory/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/042 - The Wrong'Uns - Rothko/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/043 - Timboz - Pony/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/044 - Tom McKenzie - Directions/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/045 - Traffic Experiment - Sirens/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/046 - Triviul - Dorothy/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/047 - Voelund - Comfort Lives In Belief/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/048 - We Fell From The Sky - Not You/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/049 - Young Griffo - Facade/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/050 - Zeno - Signs/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/031 - Phre The Eon - Everybody's Falling Apart/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/032 - Raft Monk - Tiring/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/033 - Sambasevam Shanmugam - Kaathaadi/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/034 - Secretariat - Over The Top/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/035 - Signe Jakobsen - What Have You Done To Me/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/036 - Skelpolu - Resurrection/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/037 - Speak Softly - Broken Man/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/038 - Spike Mullings - Mike's Sulking/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/022 - Johnny Lokke - Promises & Lies/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/023 - Jokers, Jacks & Kings - Sea Of Leaves/mixture.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Audio for mix data/DSD100/Mixtures/Test/024 - Leaf - Come Around/remainingmix.wav exceeds [-1,1] float range!\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/024 - Leaf - Come Around/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/025 - Leaf - Wicked/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/026 - Louis Cressy Band - Good Time/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/027 - M.E.R.C. Music - Knockout/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/028 - Motor Tapes - Shore/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/029 - Nerve 9 - Pray For The Rain/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/013 - Drumtracks - Ghost Bitch/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/014 - Fergessen - Back From The Start/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/015 - Fergessen - The Wind/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/016 - Forkupines - Semantics/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/017 - Girls Under Glass - We Feel Alright/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/018 - Hollow Ground - Ill Fate/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/019 - James Elder & Mark M Thompson - The English Actor/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/020 - James May - Dont Let Go/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/021 - James May - On The Line/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/030 - Patrick Talbot - A Reason To Leave/mixture.wav\n",
      "Wrote accompaniment for song data/DSD100/Mixtures/Test/039 - Swinging Steaks - Lost My Way/mixture.wav\n"
     ]
    }
   ],
   "source": [
    "dsd_train, dsd_test = Datasets.getDSDFilelist(\"DSD100.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dict()\n",
    "dataset[\"train_sup\"] = dsd_train # 50 training tracks from DSD100 as supervised dataset\n",
    "dataset[\"train_unsup\"] = [list(), list(), list()] # Initialise unsupervised dateaset structure (fill up later)\n",
    "dataset[\"valid\"] = [dsd_test[0][:25], dsd_test[1][:25], dsd_test[2][:25]] # Validation and test contains 25 songs of DSD each, plus more (added later)\n",
    "dataset[\"test\"] = [dsd_test[0][25:], dsd_test[1][25:], dsd_test[2][25:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': [[<Sample.Sample at 0x7fd856f5ae90>,\n",
       "   <Sample.Sample at 0x7fd856f5af10>,\n",
       "   <Sample.Sample at 0x7fd856f5ae10>,\n",
       "   <Sample.Sample at 0x7fd856f870d0>,\n",
       "   <Sample.Sample at 0x7fd856f87190>,\n",
       "   <Sample.Sample at 0x7fd856f87250>,\n",
       "   <Sample.Sample at 0x7fd856f87310>,\n",
       "   <Sample.Sample at 0x7fd856f87210>,\n",
       "   <Sample.Sample at 0x7fd856f87490>,\n",
       "   <Sample.Sample at 0x7fd856f87590>,\n",
       "   <Sample.Sample at 0x7fd856f87610>,\n",
       "   <Sample.Sample at 0x7fd856f876d0>,\n",
       "   <Sample.Sample at 0x7fd856f87790>,\n",
       "   <Sample.Sample at 0x7fd856f87850>,\n",
       "   <Sample.Sample at 0x7fd856f878d0>,\n",
       "   <Sample.Sample at 0x7fd856f87a10>,\n",
       "   <Sample.Sample at 0x7fd856f87a90>,\n",
       "   <Sample.Sample at 0x7fd856f87b50>,\n",
       "   <Sample.Sample at 0x7fd856f87c10>,\n",
       "   <Sample.Sample at 0x7fd856f87c90>,\n",
       "   <Sample.Sample at 0x7fd856f87cd0>,\n",
       "   <Sample.Sample at 0x7fd856f87e50>,\n",
       "   <Sample.Sample at 0x7fd856f87f10>,\n",
       "   <Sample.Sample at 0x7fd856f87e10>,\n",
       "   <Sample.Sample at 0x7fd856f660d0>],\n",
       "  [<Sample.Sample at 0x7fd856f5aed0>,\n",
       "   <Sample.Sample at 0x7fd856f5af50>,\n",
       "   <Sample.Sample at 0x7fd856f5afd0>,\n",
       "   <Sample.Sample at 0x7fd856f87110>,\n",
       "   <Sample.Sample at 0x7fd856f871d0>,\n",
       "   <Sample.Sample at 0x7fd856f87290>,\n",
       "   <Sample.Sample at 0x7fd856f87350>,\n",
       "   <Sample.Sample at 0x7fd856f87410>,\n",
       "   <Sample.Sample at 0x7fd856f874d0>,\n",
       "   <Sample.Sample at 0x7fd856f875d0>,\n",
       "   <Sample.Sample at 0x7fd856f87650>,\n",
       "   <Sample.Sample at 0x7fd856f87710>,\n",
       "   <Sample.Sample at 0x7fd856f877d0>,\n",
       "   <Sample.Sample at 0x7fd856f87890>,\n",
       "   <Sample.Sample at 0x7fd856f87950>,\n",
       "   <Sample.Sample at 0x7fd856f87a50>,\n",
       "   <Sample.Sample at 0x7fd856f87ad0>,\n",
       "   <Sample.Sample at 0x7fd856f87b90>,\n",
       "   <Sample.Sample at 0x7fd856f87c50>,\n",
       "   <Sample.Sample at 0x7fd856f87d10>,\n",
       "   <Sample.Sample at 0x7fd856f87dd0>,\n",
       "   <Sample.Sample at 0x7fd856f87e90>,\n",
       "   <Sample.Sample at 0x7fd856f87f50>,\n",
       "   <Sample.Sample at 0x7fd856f87fd0>,\n",
       "   <Sample.Sample at 0x7fd856f66110>],\n",
       "  [<Sample.Sample at 0x7fd856f5ae50>,\n",
       "   <Sample.Sample at 0x7fd856f5ad50>,\n",
       "   <Sample.Sample at 0x7fd856f5ac10>,\n",
       "   <Sample.Sample at 0x7fd856f5af90>,\n",
       "   <Sample.Sample at 0x7fd856f87050>,\n",
       "   <Sample.Sample at 0x7fd856f87090>,\n",
       "   <Sample.Sample at 0x7fd856f87150>,\n",
       "   <Sample.Sample at 0x7fd856f872d0>,\n",
       "   <Sample.Sample at 0x7fd856f87390>,\n",
       "   <Sample.Sample at 0x7fd856f87550>,\n",
       "   <Sample.Sample at 0x7fd856f873d0>,\n",
       "   <Sample.Sample at 0x7fd856f87510>,\n",
       "   <Sample.Sample at 0x7fd856f87450>,\n",
       "   <Sample.Sample at 0x7fd856f87690>,\n",
       "   <Sample.Sample at 0x7fd856f87810>,\n",
       "   <Sample.Sample at 0x7fd856f87910>,\n",
       "   <Sample.Sample at 0x7fd856f879d0>,\n",
       "   <Sample.Sample at 0x7fd856f87750>,\n",
       "   <Sample.Sample at 0x7fd856f87990>,\n",
       "   <Sample.Sample at 0x7fd856f87bd0>,\n",
       "   <Sample.Sample at 0x7fd856f87d50>,\n",
       "   <Sample.Sample at 0x7fd856f87d90>,\n",
       "   <Sample.Sample at 0x7fd856f87b10>,\n",
       "   <Sample.Sample at 0x7fd856f87f90>,\n",
       "   <Sample.Sample at 0x7fd856f87ed0>]],\n",
       " 'train_sup': [[<Sample.Sample at 0x7fd858411390>,\n",
       "   <Sample.Sample at 0x7fd858411610>,\n",
       "   <Sample.Sample at 0x7fd8584116d0>,\n",
       "   <Sample.Sample at 0x7fd858411790>,\n",
       "   <Sample.Sample at 0x7fd858411850>,\n",
       "   <Sample.Sample at 0x7fd858411910>,\n",
       "   <Sample.Sample at 0x7fd8584119d0>,\n",
       "   <Sample.Sample at 0x7fd858411a90>,\n",
       "   <Sample.Sample at 0x7fd858411b50>,\n",
       "   <Sample.Sample at 0x7fd858411c10>,\n",
       "   <Sample.Sample at 0x7fd858411b10>,\n",
       "   <Sample.Sample at 0x7fd858411d90>,\n",
       "   <Sample.Sample at 0x7fd858411e90>,\n",
       "   <Sample.Sample at 0x7fd858411f10>,\n",
       "   <Sample.Sample at 0x7fd858411e10>,\n",
       "   <Sample.Sample at 0x7fd8570bd110>,\n",
       "   <Sample.Sample at 0x7fd8570bd190>,\n",
       "   <Sample.Sample at 0x7fd8570bd250>,\n",
       "   <Sample.Sample at 0x7fd8570bd090>,\n",
       "   <Sample.Sample at 0x7fd8570bd3d0>,\n",
       "   <Sample.Sample at 0x7fd8570bd490>,\n",
       "   <Sample.Sample at 0x7fd8570bd550>,\n",
       "   <Sample.Sample at 0x7fd8570bd450>,\n",
       "   <Sample.Sample at 0x7fd8570bd6d0>,\n",
       "   <Sample.Sample at 0x7fd8570bd790>,\n",
       "   <Sample.Sample at 0x7fd8570bd850>,\n",
       "   <Sample.Sample at 0x7fd8570bd910>,\n",
       "   <Sample.Sample at 0x7fd8570bda10>,\n",
       "   <Sample.Sample at 0x7fd8570bda90>,\n",
       "   <Sample.Sample at 0x7fd8570bdb50>,\n",
       "   <Sample.Sample at 0x7fd8570bd810>,\n",
       "   <Sample.Sample at 0x7fd8570bdc50>,\n",
       "   <Sample.Sample at 0x7fd8570bdd90>,\n",
       "   <Sample.Sample at 0x7fd8570bde50>,\n",
       "   <Sample.Sample at 0x7fd8570bdf50>,\n",
       "   <Sample.Sample at 0x7fd8570bde10>,\n",
       "   <Sample.Sample at 0x7fd8570bc0d0>,\n",
       "   <Sample.Sample at 0x7fd8570bc150>,\n",
       "   <Sample.Sample at 0x7fd8570bc190>,\n",
       "   <Sample.Sample at 0x7fd8570bc310>,\n",
       "   <Sample.Sample at 0x7fd8570bc3d0>,\n",
       "   <Sample.Sample at 0x7fd8570bc490>,\n",
       "   <Sample.Sample at 0x7fd8570bc550>,\n",
       "   <Sample.Sample at 0x7fd8570bc610>,\n",
       "   <Sample.Sample at 0x7fd8570bc710>,\n",
       "   <Sample.Sample at 0x7fd8570bc5d0>,\n",
       "   <Sample.Sample at 0x7fd8570bc790>,\n",
       "   <Sample.Sample at 0x7fd8570bc850>,\n",
       "   <Sample.Sample at 0x7fd8570bc9d0>,\n",
       "   <Sample.Sample at 0x7fd8570bca90>],\n",
       "  [<Sample.Sample at 0x7fd858411490>,\n",
       "   <Sample.Sample at 0x7fd858411650>,\n",
       "   <Sample.Sample at 0x7fd858411710>,\n",
       "   <Sample.Sample at 0x7fd8584117d0>,\n",
       "   <Sample.Sample at 0x7fd858411890>,\n",
       "   <Sample.Sample at 0x7fd858411950>,\n",
       "   <Sample.Sample at 0x7fd858411a10>,\n",
       "   <Sample.Sample at 0x7fd858411ad0>,\n",
       "   <Sample.Sample at 0x7fd858411b90>,\n",
       "   <Sample.Sample at 0x7fd858411c50>,\n",
       "   <Sample.Sample at 0x7fd858411d10>,\n",
       "   <Sample.Sample at 0x7fd858411dd0>,\n",
       "   <Sample.Sample at 0x7fd858411ed0>,\n",
       "   <Sample.Sample at 0x7fd858411f50>,\n",
       "   <Sample.Sample at 0x7fd858411fd0>,\n",
       "   <Sample.Sample at 0x7fd8570bd150>,\n",
       "   <Sample.Sample at 0x7fd8570bd1d0>,\n",
       "   <Sample.Sample at 0x7fd8570bd290>,\n",
       "   <Sample.Sample at 0x7fd8570bd350>,\n",
       "   <Sample.Sample at 0x7fd8570bd410>,\n",
       "   <Sample.Sample at 0x7fd8570bd4d0>,\n",
       "   <Sample.Sample at 0x7fd8570bd590>,\n",
       "   <Sample.Sample at 0x7fd8570bd650>,\n",
       "   <Sample.Sample at 0x7fd8570bd710>,\n",
       "   <Sample.Sample at 0x7fd8570bd7d0>,\n",
       "   <Sample.Sample at 0x7fd8570bd890>,\n",
       "   <Sample.Sample at 0x7fd8570bd950>,\n",
       "   <Sample.Sample at 0x7fd8570bda50>,\n",
       "   <Sample.Sample at 0x7fd8570bdad0>,\n",
       "   <Sample.Sample at 0x7fd8570bdb90>,\n",
       "   <Sample.Sample at 0x7fd8570bdc90>,\n",
       "   <Sample.Sample at 0x7fd8570bdd10>,\n",
       "   <Sample.Sample at 0x7fd8570bddd0>,\n",
       "   <Sample.Sample at 0x7fd8570bde90>,\n",
       "   <Sample.Sample at 0x7fd8570bdf90>,\n",
       "   <Sample.Sample at 0x7fd8570bdfd0>,\n",
       "   <Sample.Sample at 0x7fd8570bc110>,\n",
       "   <Sample.Sample at 0x7fd8570bc1d0>,\n",
       "   <Sample.Sample at 0x7fd8570bc290>,\n",
       "   <Sample.Sample at 0x7fd8570bc350>,\n",
       "   <Sample.Sample at 0x7fd8570bc410>,\n",
       "   <Sample.Sample at 0x7fd8570bc390>,\n",
       "   <Sample.Sample at 0x7fd8570bc590>,\n",
       "   <Sample.Sample at 0x7fd8570bc650>,\n",
       "   <Sample.Sample at 0x7fd8570bc750>,\n",
       "   <Sample.Sample at 0x7fd8570bc7d0>,\n",
       "   <Sample.Sample at 0x7fd8570bc890>,\n",
       "   <Sample.Sample at 0x7fd8570bc950>,\n",
       "   <Sample.Sample at 0x7fd8570bca10>,\n",
       "   <Sample.Sample at 0x7fd8570bcad0>],\n",
       "  [<Sample.Sample at 0x7fd8584c6dd0>,\n",
       "   <Sample.Sample at 0x7fd858411550>,\n",
       "   <Sample.Sample at 0x7fd858411590>,\n",
       "   <Sample.Sample at 0x7fd8584115d0>,\n",
       "   <Sample.Sample at 0x7fd858411690>,\n",
       "   <Sample.Sample at 0x7fd858411750>,\n",
       "   <Sample.Sample at 0x7fd858411810>,\n",
       "   <Sample.Sample at 0x7fd8584118d0>,\n",
       "   <Sample.Sample at 0x7fd858411990>,\n",
       "   <Sample.Sample at 0x7fd858411a50>,\n",
       "   <Sample.Sample at 0x7fd858411bd0>,\n",
       "   <Sample.Sample at 0x7fd858411c90>,\n",
       "   <Sample.Sample at 0x7fd858411e50>,\n",
       "   <Sample.Sample at 0x7fd858411cd0>,\n",
       "   <Sample.Sample at 0x7fd858411d50>,\n",
       "   <Sample.Sample at 0x7fd858411f90>,\n",
       "   <Sample.Sample at 0x7fd8570bd050>,\n",
       "   <Sample.Sample at 0x7fd8570bd0d0>,\n",
       "   <Sample.Sample at 0x7fd8570bd210>,\n",
       "   <Sample.Sample at 0x7fd8570bd2d0>,\n",
       "   <Sample.Sample at 0x7fd8570bd390>,\n",
       "   <Sample.Sample at 0x7fd8570bd310>,\n",
       "   <Sample.Sample at 0x7fd8570bd510>,\n",
       "   <Sample.Sample at 0x7fd8570bd5d0>,\n",
       "   <Sample.Sample at 0x7fd8570bd690>,\n",
       "   <Sample.Sample at 0x7fd8570bd610>,\n",
       "   <Sample.Sample at 0x7fd8570bd750>,\n",
       "   <Sample.Sample at 0x7fd8570bd9d0>,\n",
       "   <Sample.Sample at 0x7fd8570bd8d0>,\n",
       "   <Sample.Sample at 0x7fd8570bd990>,\n",
       "   <Sample.Sample at 0x7fd8570bdc10>,\n",
       "   <Sample.Sample at 0x7fd8570bdb10>,\n",
       "   <Sample.Sample at 0x7fd8570bdbd0>,\n",
       "   <Sample.Sample at 0x7fd8570bdcd0>,\n",
       "   <Sample.Sample at 0x7fd8570bdf10>,\n",
       "   <Sample.Sample at 0x7fd8570bded0>,\n",
       "   <Sample.Sample at 0x7fd8570bdd50>,\n",
       "   <Sample.Sample at 0x7fd8570bc090>,\n",
       "   <Sample.Sample at 0x7fd8570bc210>,\n",
       "   <Sample.Sample at 0x7fd8570bc250>,\n",
       "   <Sample.Sample at 0x7fd8570bc050>,\n",
       "   <Sample.Sample at 0x7fd8570bc2d0>,\n",
       "   <Sample.Sample at 0x7fd8570bc4d0>,\n",
       "   <Sample.Sample at 0x7fd8570bc510>,\n",
       "   <Sample.Sample at 0x7fd8570bc6d0>,\n",
       "   <Sample.Sample at 0x7fd8570bc690>,\n",
       "   <Sample.Sample at 0x7fd8570bc810>,\n",
       "   <Sample.Sample at 0x7fd8570bc8d0>,\n",
       "   <Sample.Sample at 0x7fd8570bc910>,\n",
       "   <Sample.Sample at 0x7fd8570bc450>]],\n",
       " 'train_unsup': [[], [], []],\n",
       " 'valid': [[<Sample.Sample at 0x7fd8570bcb50>,\n",
       "   <Sample.Sample at 0x7fd8570bcc10>,\n",
       "   <Sample.Sample at 0x7fd8570bcb10>,\n",
       "   <Sample.Sample at 0x7fd8570bcc90>,\n",
       "   <Sample.Sample at 0x7fd8570bce50>,\n",
       "   <Sample.Sample at 0x7fd8570bcf10>,\n",
       "   <Sample.Sample at 0x7fd8570bcd90>,\n",
       "   <Sample.Sample at 0x7fd856f5a110>,\n",
       "   <Sample.Sample at 0x7fd856f5a1d0>,\n",
       "   <Sample.Sample at 0x7fd856f5a250>,\n",
       "   <Sample.Sample at 0x7fd856f5a310>,\n",
       "   <Sample.Sample at 0x7fd856f5a3d0>,\n",
       "   <Sample.Sample at 0x7fd856f5a4d0>,\n",
       "   <Sample.Sample at 0x7fd856f5a550>,\n",
       "   <Sample.Sample at 0x7fd856f5a6d0>,\n",
       "   <Sample.Sample at 0x7fd856f5a2d0>,\n",
       "   <Sample.Sample at 0x7fd856f5a790>,\n",
       "   <Sample.Sample at 0x7fd856f5a450>,\n",
       "   <Sample.Sample at 0x7fd856f5a910>,\n",
       "   <Sample.Sample at 0x7fd856f5a9d0>,\n",
       "   <Sample.Sample at 0x7fd856f5ab10>,\n",
       "   <Sample.Sample at 0x7fd856f5aa90>,\n",
       "   <Sample.Sample at 0x7fd856f5aa50>,\n",
       "   <Sample.Sample at 0x7fd856f5acd0>,\n",
       "   <Sample.Sample at 0x7fd856f5ad90>],\n",
       "  [<Sample.Sample at 0x7fd8570bcb90>,\n",
       "   <Sample.Sample at 0x7fd8570bcc50>,\n",
       "   <Sample.Sample at 0x7fd8570bcd10>,\n",
       "   <Sample.Sample at 0x7fd8570bcdd0>,\n",
       "   <Sample.Sample at 0x7fd8570bce90>,\n",
       "   <Sample.Sample at 0x7fd8570bcf50>,\n",
       "   <Sample.Sample at 0x7fd8570bcfd0>,\n",
       "   <Sample.Sample at 0x7fd856f5a150>,\n",
       "   <Sample.Sample at 0x7fd856f5a210>,\n",
       "   <Sample.Sample at 0x7fd856f5a290>,\n",
       "   <Sample.Sample at 0x7fd856f5a350>,\n",
       "   <Sample.Sample at 0x7fd856f5a410>,\n",
       "   <Sample.Sample at 0x7fd856f5a510>,\n",
       "   <Sample.Sample at 0x7fd856f5a590>,\n",
       "   <Sample.Sample at 0x7fd856f5a710>,\n",
       "   <Sample.Sample at 0x7fd856f5a690>,\n",
       "   <Sample.Sample at 0x7fd856f5a7d0>,\n",
       "   <Sample.Sample at 0x7fd856f5a890>,\n",
       "   <Sample.Sample at 0x7fd856f5a950>,\n",
       "   <Sample.Sample at 0x7fd856f5aa10>,\n",
       "   <Sample.Sample at 0x7fd856f5ab50>,\n",
       "   <Sample.Sample at 0x7fd856f5ab90>,\n",
       "   <Sample.Sample at 0x7fd856f5ac50>,\n",
       "   <Sample.Sample at 0x7fd856f5ad10>,\n",
       "   <Sample.Sample at 0x7fd856f5add0>],\n",
       "  [<Sample.Sample at 0x7fd8570bca50>,\n",
       "   <Sample.Sample at 0x7fd8570bc990>,\n",
       "   <Sample.Sample at 0x7fd8570bcbd0>,\n",
       "   <Sample.Sample at 0x7fd8570bcd50>,\n",
       "   <Sample.Sample at 0x7fd8570bccd0>,\n",
       "   <Sample.Sample at 0x7fd8570bce10>,\n",
       "   <Sample.Sample at 0x7fd8570bced0>,\n",
       "   <Sample.Sample at 0x7fd8570bcf90>,\n",
       "   <Sample.Sample at 0x7fd856f5a190>,\n",
       "   <Sample.Sample at 0x7fd856f5a0d0>,\n",
       "   <Sample.Sample at 0x7fd856f5a090>,\n",
       "   <Sample.Sample at 0x7fd856f5a050>,\n",
       "   <Sample.Sample at 0x7fd856f5a490>,\n",
       "   <Sample.Sample at 0x7fd856f5a390>,\n",
       "   <Sample.Sample at 0x7fd856f5a5d0>,\n",
       "   <Sample.Sample at 0x7fd856f5a610>,\n",
       "   <Sample.Sample at 0x7fd856f5a650>,\n",
       "   <Sample.Sample at 0x7fd856f5a750>,\n",
       "   <Sample.Sample at 0x7fd856f5a810>,\n",
       "   <Sample.Sample at 0x7fd856f5a8d0>,\n",
       "   <Sample.Sample at 0x7fd856f5aad0>,\n",
       "   <Sample.Sample at 0x7fd856f5a990>,\n",
       "   <Sample.Sample at 0x7fd856f5a850>,\n",
       "   <Sample.Sample at 0x7fd856f5abd0>,\n",
       "   <Sample.Sample at 0x7fd856f5ac90>]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d335cc2196a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#ex = Experiment('Drum_Source_Separation')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     model_config = {\"model_base_dir\" : \"checkpoints\", # Base folder for model checkpoints\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ex' is not defined"
     ]
    }
   ],
   "source": [
    "ex = Experiment('Drum_Source_Separation')\n",
    "\n",
    "@ex.config\n",
    "def cfg():\n",
    "    model_config = {\"model_base_dir\" : \"checkpoints\", # Base folder for model checkpoints\n",
    "                    \"log_dir\" : \"logs\", # Base folder for logs files\n",
    "                    \"batch_size\" : 64, # Batch size\n",
    "                    \"alpha\" : 0.001, # Weighting for adversarial loss (unsupervised)\n",
    "                    \"beta\" : 0.001, # Weighting for additive penalty (unsupervised)\n",
    "                    \"lam\" : 10, # Weighting term lambda for WGAN gradient penalty\n",
    "                    \"init_disc_lr\" : 5e-5, # Discriminator(s) learning rate\n",
    "                    \"init_sup_sep_lr\" : 5e-5, # Supervised separator learning rate\n",
    "                    \"init_unsup_sep_lr\" : 5e-5, # Unsupervised separator learning rate\n",
    "                    \"epoch_it\" : 1000, # Number of supervised separator steps per epoch\n",
    "                    \"num_disc\": 5,  # Number of discriminator iterations per separator update\n",
    "                    \"num_frames\" : 64, # DESIRED number of time frames in the spectrogram per sample (this can be increased when using U-net due to its limited output sizes)\n",
    "                    \"num_fft\" : 512, # FFT Size\n",
    "                    \"num_hop\" : 256, # FFT Hop size\n",
    "                    'expected_sr' : 8192, # Downsample all audio input to this sampling rate\n",
    "                    'mono_downmix' : True, # Whether to downsample the audio input\n",
    "                    'cache_size' : 64, # Number of audio excerpts that are cached to build batches from\n",
    "                    'num_workers' : 4, # Number of processes reading audio and filling up the cache\n",
    "                    \"duration\" : 10, # Duration in seconds of the audio excerpts in the cache (excluding input context)\n",
    "                    'min_replacement_rate' : 8,  # roughly: how many cache entries to replace at least per batch on average. Can be fractional\n",
    "                    'num_layers' : 4, # How many U-Net layers\n",
    "                    }\n",
    "\n",
    "    experiment_id = np.random.randint(0,1000000)\n",
    "\n",
    "@ex.capture\n",
    "def test(model_config, audio_list, model_folder, load_model):\n",
    "    # Determine input and output shapes, if we use U-net as separator\n",
    "    freq_bins = model_config[\"num_fft\"] / 2 + 1  # Make even number of freq bins\n",
    "    disc_input_shape = [model_config[\"batch_size\"], freq_bins-1, model_config[\"num_frames\"],1]  # Shape of discriminator input\n",
    "    separator_class = Models.Unet.Unet(model_config[\"num_layers\"])\n",
    "    sep_input_shape, sep_output_shape = separator_class.getUnetPadding(np.array(disc_input_shape))\n",
    "    separator_func = separator_class.get_output\n",
    "\n",
    "    # Placeholders and input normalisation\n",
    "    input_ph, queue, [mix_context, acc, drums] = Input.get_multitrack_input(sep_output_shape[1:], model_config[\"batch_size\"], name=\"input_batch\", input_shape=sep_input_shape[1:])\n",
    "    enqueue_op = queue.enqueue(input_ph)\n",
    "\n",
    "    mix = Input.crop(mix_context, sep_output_shape)\n",
    "    mix_norm, mix_context_norm, acc_norm, drum_norm = Input.norm(mix), Input.norm(mix_context), Input.norm(acc), Input.norm(drums)\n",
    "\n",
    "    print(\"Testing...\")\n",
    "\n",
    "    # BUILD MODELS\n",
    "    # Separator\n",
    "    separator_acc_norm, separator_drums_norm = separator_func(mix_context_norm, reuse=False)\n",
    "\n",
    "    # Supervised objective\n",
    "    sup_separator_loss = tf.reduce_mean(tf.square(separator_drums_norm - drum_norm)) + tf.reduce_mean(tf.square(separator_acc_norm - acc_norm))\n",
    "\n",
    "    tf.summary.scalar(\"sup_sep_loss\", sup_separator_loss, collections=['sup', 'unsup'])\n",
    "\n",
    "    global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False, dtype=tf.int64)\n",
    "\n",
    "    # Start session and queue input threads\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(model_config[\"log_dir\"] + os.path.sep +  model_folder, graph=sess.graph)\n",
    "\n",
    "    thread = threading.Thread(target=Input.load_and_enqueue, args=(sess, model_config, queue, enqueue_op, input_ph, audio_list))\n",
    "    thread.deamon = True\n",
    "    thread.start()\n",
    "\n",
    "    # CHECKPOINTING\n",
    "    # Load pretrained model to test\n",
    "    restorer = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2)\n",
    "    print(\"Num of variables\" + str(len(tf.global_variables())))\n",
    "    restorer.restore(sess, load_model)\n",
    "    print('Pre-trained model restored for testing')\n",
    "\n",
    "    # Start training loop\n",
    "    _global_step = sess.run(global_step)\n",
    "    print(\"Starting!\")\n",
    "    batches = 0\n",
    "    total_loss = 0.0\n",
    "    run = True\n",
    "    while run:\n",
    "        try:\n",
    "            _sup_separator_loss = sess.run(\n",
    "               sup_separator_loss)\n",
    "            total_loss += _sup_separator_loss # Aggregate loss measure\n",
    "            batches += 1\n",
    "        except Exception as e:\n",
    "            print(\"Emptied queue - finished this epoch!\")\n",
    "            run = False\n",
    "\n",
    "    mean_mse_loss = total_loss / float(batches)\n",
    "    summary = tf.Summary(value=[tf.Summary.Value(tag=\"test_loss\", simple_value=mean_mse_loss)])\n",
    "    writer.add_summary(summary, global_step=_global_step)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    print(\"Finished testing - Mean MSE: \" + str(mean_mse_loss))\n",
    "\n",
    "    thread.join()\n",
    "\n",
    "    # Close session, clear computational graph\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    return mean_mse_loss\n",
    "\n",
    "\n",
    "@ex.capture\n",
    "def train(model_config, sup_dataset, model_folder, unsup_dataset=None, load_model=None):\n",
    "    # Determine input and output shapes\n",
    "    freq_bins = model_config[\"num_fft\"] / 2 + 1  # Make even number of freq bins\n",
    "    disc_input_shape = [model_config[\"batch_size\"], freq_bins - 1, model_config[\"num_frames\"],1]  # Shape of discriminator input\n",
    "\n",
    "    separator_class = Models.Unet.Unet(model_config[\"num_layers\"])\n",
    "    sep_input_shape, sep_output_shape = separator_class.getUnetPadding(np.array(disc_input_shape))\n",
    "    separator_func = separator_class.get_output\n",
    "\n",
    "    # Batch input workers\n",
    "    # Creating the batch generators\n",
    "    padding_durations = [float(sep_input_shape[2] - sep_output_shape[2]) * model_config[\"num_hop\"] / model_config[\"expected_sr\"] / 2.0, 0, 0]  # Input context that the input audio has to be padded with while reading audio files\n",
    "    sup_batch_gen = batchgen.BatchGen_Paired(\n",
    "        model_config,\n",
    "        sup_dataset,\n",
    "        sep_input_shape,\n",
    "        sep_output_shape,\n",
    "        padding_durations[0]\n",
    "    )\n",
    "\n",
    "    # Creating unsupervised batch generator if needed\n",
    "    if unsup_dataset != None:\n",
    "        unsup_batch_gens = list()\n",
    "        for i in range(3):\n",
    "            shape = (sep_input_shape if i==0 else sep_output_shape)\n",
    "            unsup_batch_gens.append(batchgen.BatchGen_Single(\n",
    "                model_config,\n",
    "                unsup_dataset[i],\n",
    "                shape,\n",
    "                padding_durations[i]\n",
    "            ))\n",
    "\n",
    "    print(\"Starting worker\")\n",
    "    sup_batch_gen.start_workers()\n",
    "    print(\"Started worker!\")\n",
    "\n",
    "    if unsup_dataset != None:\n",
    "        for gen in unsup_batch_gens:\n",
    "            print(\"Starting worker\")\n",
    "            gen.start_workers()\n",
    "            print(\"Started worker!\")\n",
    "\n",
    "    # Placeholders and input normalisation\n",
    "    mix_context,acc,drums = Input.get_multitrack_placeholders(sep_output_shape, sep_input_shape, \"sup\")\n",
    "    mix = Input.crop(mix_context, sep_output_shape)\n",
    "    mix_norm, mix_context_norm, acc_norm, drums_norm = Input.norm(mix), Input.norm(mix_context), Input.norm(acc), Input.norm(drums)\n",
    "\n",
    "    if unsup_dataset != None:\n",
    "        mix_context_u,acc_u,drums_u = Input.get_multitrack_placeholders(sep_output_shape, sep_input_shape, \"unsup\")\n",
    "        mix_u = Input.crop(mix_context_u, sep_output_shape)\n",
    "        mix_norm_u, mix_context_norm_u, acc_norm_u, drums_norm_u = Input.norm(mix_u), Input.norm(mix_context_u), Input.norm(acc_u), Input.norm(drums_u)\n",
    "\n",
    "    print(\"Training...\")\n",
    "\n",
    "    # BUILD MODELS\n",
    "    # Separator\n",
    "    separator_acc_norm, separator_drums_norm = separator_func(mix_context_norm, reuse=False)\n",
    "    separator_acc, separator_drums = Input.denorm(separator_acc_norm), Input.denorm(separator_drums_norm)\n",
    "    if unsup_dataset != None:\n",
    "        separator_acc_norm_u, separator_drums_norm_u = separator_func(mix_context_norm_u, reuse=True)\n",
    "        separator_acc_u, separator_drums_u = Input.denorm(separator_acc_norm_u), Input.denorm(separator_drums_norm_u)\n",
    "        mask_loss_u = tf.reduce_mean(tf.square(mix_u - separator_acc_u - separator_drums_u))\n",
    "    mask_loss = tf.reduce_mean(tf.square(mix - separator_acc - separator_drums))\n",
    "\n",
    "    # SUMMARIES FOR INPUT AND SEPARATOR\n",
    "    tf.summary.scalar(\"mask_loss\", mask_loss, collections=[\"sup\", \"unsup\"])\n",
    "    if unsup_dataset != None:\n",
    "        tf.summary.scalar(\"mask_loss_u\", mask_loss_u, collections=[\"unsup\"])\n",
    "        tf.summary.scalar(\"acc_norm_mean_u\", tf.reduce_mean(acc_norm_u), collections=[\"acc_disc\"])\n",
    "        tf.summary.scalar(\"drums_norm_mean_u\", tf.reduce_mean(drums_norm_u), collections=[\"drums_disc\"])\n",
    "        tf.summary.scalar(\"acc_sep_norm_mean_u\", tf.reduce_mean(separator_acc_norm_u), collections=[\"acc_disc\"])\n",
    "        tf.summary.scalar(\"drums_sep_norm_mean_u\", tf.reduce_mean(separator_drums_norm_u), collections=[\"drums_disc\"])\n",
    "    tf.summary.scalar(\"acc_norm_mean\", tf.reduce_mean(acc_norm), collections=['sup'])\n",
    "    tf.summary.scalar(\"drums_norm_mean\", tf.reduce_mean(drums_norm), collections=['sup'])\n",
    "    tf.summary.scalar(\"acc_sep_norm_mean\", tf.reduce_mean(separator_acc_norm), collections=['sup'])\n",
    "    tf.summary.scalar(\"drums_sep_norm_mean\", tf.reduce_mean(separator_drums_norm), collections=['sup'])\n",
    "\n",
    "    tf.summary.image(\"sep_acc_norm\", separator_acc_norm, collections=[\"sup\", \"unsup\"])\n",
    "    tf.summary.image(\"sep_drums_norm\", separator_drums_norm, collections=[\"sup\", \"unsup\"])\n",
    "\n",
    "    # BUILD DISCRIMINATORS, if unsupervised training\n",
    "    unsup_separator_loss = 0\n",
    "    if unsup_dataset != None:\n",
    "        disc_func = Models.WGAN_Critic.dcgan\n",
    "\n",
    "        # Define real and fake inputs for both discriminators - if separator output and dsicriminator input shapes do not fit perfectly, we will do a centre crop and only discriminate that part\n",
    "        acc_real_input = Input.crop(acc_norm_u, disc_input_shape)\n",
    "        acc_fake_input = Input.crop(separator_acc_norm_u, disc_input_shape)\n",
    "        drums_real_input = Input.crop(drums_norm_u, disc_input_shape)\n",
    "        drums_fake_input = Input.crop(separator_drums_norm_u, disc_input_shape)\n",
    "\n",
    "        #WGAN\n",
    "        acc_disc_loss, acc_disc_real, acc_disc_fake, acc_grad_pen, acc_wasserstein_dist = \\\n",
    "            Models.WGAN_Critic.create_critic(model_config, real_input=acc_real_input, fake_input=acc_fake_input, scope=\"acc_disc\", network_func=disc_func)\n",
    "        drums_disc_loss, drums_disc_real, drums_disc_fake, drums_grad_pen, drums_wasserstein_dist = \\\n",
    "            Models.WGAN_Critic.create_critic(model_config, real_input=drums_real_input, fake_input=drums_fake_input, scope=\"drums_disc\", network_func=disc_func)\n",
    "\n",
    "        L_u = - tf.reduce_mean(drums_disc_fake)  - tf.reduce_mean(acc_disc_fake) # WGAN based loss for separator (L_u in paper)\n",
    "        unsup_separator_loss = model_config[\"alpha\"] * L_u + model_config[\"beta\"] * mask_loss_u # Unsupervised loss for separator: WGAN-based loss L_u and additive penalty term (mask loss), weighted by alpha and beta (hyperparameters)\n",
    "\n",
    "    # Supervised objective: MSE in log-normalized magnitude space\n",
    "    sup_separator_loss = tf.reduce_mean(tf.square(separator_drums_norm - drums_norm)) + \\\n",
    "                         tf.reduce_mean(tf.square(separator_acc_norm - acc_norm))\n",
    "\n",
    "    separator_loss = sup_separator_loss + unsup_separator_loss # Total separator loss: Supervised + unsupervised loss\n",
    "\n",
    "    # TRAINING CONTROL VARIABLES\n",
    "    global_step = tf.get_variable('global_step', [],\n",
    "                                  initializer=tf.constant_initializer(0), trainable=False, dtype=tf.int64)\n",
    "    increment_global_step = tf.assign(global_step, global_step + 1)\n",
    "    disc_lr = tf.get_variable('disc_lr', [],\n",
    "                              initializer=tf.constant_initializer(model_config[\"init_disc_lr\"], dtype=tf.float32), trainable=False)\n",
    "    unsup_sep_lr = tf.get_variable('unsup_sep_lr', [],\n",
    "                             initializer=tf.constant_initializer(model_config[\"init_unsup_sep_lr\"], dtype=tf.float32), trainable=False)\n",
    "    sup_sep_lr = tf.get_variable('sup_sep_lr', [],\n",
    "                             initializer=tf.constant_initializer(model_config[\"init_sup_sep_lr\"], dtype=tf.float32),\n",
    "                             trainable=False)\n",
    "\n",
    "    # Set up optimizers\n",
    "    separator_vars = Utils.getTrainableVariables(\"separator\")\n",
    "    print(\"Sep_Vars: \" + str(Utils.getNumParams(separator_vars)))\n",
    "\n",
    "    acc_disc_vars, drums_disc_vars = Utils.getTrainableVariables(\"acc_disc\"), Utils.getTrainableVariables(\"drums_disc\")\n",
    "    print(\"Drums_Disc_Vars: \" + str(Utils.getNumParams(drums_disc_vars)))\n",
    "    print(\"Acc_Disc_Vars: \" + str(Utils.getNumParams(acc_disc_vars)))\n",
    "\n",
    "    if unsup_dataset != None:\n",
    "        with tf.variable_scope(\"drums_disc_solver\"):\n",
    "            drums_disc_solver = tf.train.AdamOptimizer(learning_rate=disc_lr).minimize(drums_disc_loss, var_list=drums_disc_vars, colocate_gradients_with_ops=True)\n",
    "        with tf.variable_scope(\"acc_disc_solver\"):\n",
    "            acc_disc_solver = tf.train.AdamOptimizer(learning_rate=disc_lr).minimize(acc_disc_loss, var_list=acc_disc_vars, colocate_gradients_with_ops=True)\n",
    "        with tf.variable_scope(\"unsup_separator_solver\"):\n",
    "            unsup_separator_solver = tf.train.AdamOptimizer(learning_rate=unsup_sep_lr).minimize(\n",
    "                separator_loss, var_list=separator_vars, colocate_gradients_with_ops=True)\n",
    "    else:\n",
    "        with tf.variable_scope(\"separator_solver\"):\n",
    "            sup_separator_solver = (tf.train.AdamOptimizer(learning_rate=sup_sep_lr).minimize(sup_separator_loss, var_list=separator_vars, colocate_gradients_with_ops=True))\n",
    "\n",
    "    # SUMMARIES FOR DISCRIMINATORS AND LOSSES\n",
    "    acc_disc_summaries = tf.summary.merge_all(key=\"acc_disc\")\n",
    "    drums_disc_summaries = tf.summary.merge_all(key=\"drums_disc\")\n",
    "    tf.summary.scalar(\"sup_sep_loss\", sup_separator_loss, collections=['sup', \"unsup\"])\n",
    "    tf.summary.scalar(\"unsup_sep_loss\", unsup_separator_loss, collections=['unsup'])\n",
    "    tf.summary.scalar(\"sep_loss\", separator_loss, collections=[\"sup\", \"unsup\"])\n",
    "    sup_summaries = tf.summary.merge_all(key='sup')\n",
    "    unsup_summaries = tf.summary.merge_all(key='unsup')\n",
    "\n",
    "    # Start session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(model_config[\"log_dir\"] + os.path.sep + model_folder, graph=sess.graph)\n",
    "\n",
    "    # CHECKPOINTING\n",
    "    # Load pretrained model to continue training, if we are supposed to\n",
    "    if load_model != None:\n",
    "        restorer = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2)\n",
    "        print(\"Num of variables\" + str(len(tf.global_variables())))\n",
    "        restorer.restore(sess, load_model)\n",
    "        print('Pre-trained model restored from file ' + load_model)\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2)\n",
    "\n",
    "\n",
    "    # Start training loop\n",
    "    run = True\n",
    "    _global_step = sess.run(global_step)\n",
    "    _init_step = _global_step\n",
    "    it = 0\n",
    "    while run:\n",
    "        if unsup_dataset != None:\n",
    "            # TRAIN DISCRIMINATORS\n",
    "            for disc_it in range(model_config[\"num_disc\"]):\n",
    "                    batches = list()\n",
    "                    for gen in unsup_batch_gens:\n",
    "                        batches.append(gen.get_batch())\n",
    "\n",
    "                    _,  _acc_disc_summaries = sess.run(\n",
    "                        [acc_disc_solver, acc_disc_summaries],\n",
    "                        feed_dict={mix_context_u: batches[0], acc_u: batches[1]}\n",
    "                    )\n",
    "\n",
    "                    _,  _drums_disc_summaries = sess.run(\n",
    "                        [drums_disc_solver, drums_disc_summaries],\n",
    "                        feed_dict={mix_context_u: batches[0], drums_u: batches[2]}\n",
    "                    )\n",
    "\n",
    "                    writer.add_summary(_acc_disc_summaries, global_step=it)\n",
    "                    writer.add_summary(_drums_disc_summaries, global_step=it)\n",
    "\n",
    "                    it += 1\n",
    "\n",
    "        # TRAIN SEPARATOR\n",
    "        sup_batch = sup_batch_gen.get_batch()\n",
    "\n",
    "        if unsup_dataset != None:\n",
    "            # SUP + UNSUPERVISED TRAINING\n",
    "            unsup_batches = list()\n",
    "            for gen in unsup_batch_gens:\n",
    "                unsup_batches.append(gen.get_batch())\n",
    "\n",
    "            _, _unsup_summaries, _sup_summaries = sess.run(\n",
    "                [unsup_separator_solver, unsup_summaries, sup_summaries],\n",
    "                feed_dict={mix_context: sup_batch[0], acc: sup_batch[1], drums: sup_batch[2],\n",
    "                           mix_context_u: unsup_batches[0], acc_u:unsup_batches[1], drums_u:unsup_batches[2]}\n",
    "            )\n",
    "            writer.add_summary(_unsup_summaries, global_step=_global_step)\n",
    "        else:\n",
    "            # PURELY SUPERVISED TRAINING\n",
    "            _, _sup_summaries = sess.run(\n",
    "               [sup_separator_solver, sup_summaries],\n",
    "                feed_dict={mix_context: sup_batch[0], acc: sup_batch[1], drums: sup_batch[2]})\n",
    "            writer.add_summary(_sup_summaries, global_step=_global_step)\n",
    "\n",
    "        # Increment step counter, check if maximum iterations per epoch is achieved and stop in that case\n",
    "        _global_step = sess.run(increment_global_step)\n",
    "\n",
    "        if _global_step - _init_step > model_config[\"epoch_it\"]:\n",
    "            run = False\n",
    "            print(\"Finished training phase, stopping batch generators\")\n",
    "            sup_batch_gen.stop_workers()\n",
    "\n",
    "            if unsup_dataset != None:\n",
    "                for gen in unsup_batch_gens:\n",
    "                    gen.stop_workers()\n",
    "\n",
    "    # Epoch finished - Save model\n",
    "    print(\"Finished epoch!\")\n",
    "    save_path = saver.save(sess, model_config[\"model_base_dir\"] + os.path.sep + model_folder + os.path.sep + model_folder, global_step=int(_global_step))\n",
    "\n",
    "    # Close session, clear computational graph\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    return save_path\n",
    "\n",
    "@ex.capture\n",
    "def optimise(experiment_id, dataset, supervised):\n",
    "    '''\n",
    "    Performs either supervised or unsupervised training of the separation system.\n",
    "    Training stops if validation loss did not improve for a number of epochs, then final performance on test set is determined \n",
    "    :param dataset: Dataset dict containing the supervised, unsupervised, valiation and test partition\n",
    "    :param supervised: Boolean, whether to train supervised or semi-supervised\n",
    "    :return: [path to checkpoint file of best model, test loss of best model]\n",
    "    '''\n",
    "    if supervised:\n",
    "        unsup_dataset = None\n",
    "        model_folder = str(experiment_id) + \"_sup\"\n",
    "    else:\n",
    "        model_folder = str(experiment_id) + \"_semisup\"\n",
    "        unsup_dataset = dataset[\"train_unsup\"]\n",
    "\n",
    "    epoch = 0\n",
    "    best_loss = 10000\n",
    "    model_path = None\n",
    "    worse_epochs = 0\n",
    "    best_model_path = \"\"\n",
    "    while worse_epochs < 1: # Early stopping on validation set after a few epochs\n",
    "        print(\"EPOCH: \" + str(epoch))\n",
    "        model_path = train(sup_dataset=dataset[\"train_sup\"], unsup_dataset=unsup_dataset, model_folder=model_folder, load_model=model_path)\n",
    "        curr_loss = test(audio_list=dataset[\"valid\"], model_folder=model_folder, load_model=model_path)\n",
    "        epoch += 1\n",
    "        if curr_loss < best_loss:\n",
    "            worse_epochs = 0\n",
    "            print(\"Performance on validation set improved from \" + str(best_loss) + \" to \" + str(curr_loss))\n",
    "            best_model_path = model_path\n",
    "            best_loss = curr_loss\n",
    "        else:\n",
    "            worse_epochs += 1\n",
    "            print(\"Performance on validation set worsened to \" + str(curr_loss))\n",
    "    print(\"TRAINING FINISHED - TESTING WITH BEST MODEL \" + best_model_path)\n",
    "    test_loss = test(audio_list=dataset[\"test\"], model_folder=model_folder, load_model=best_model_path)\n",
    "    return best_model_path, test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__main__.pysndfile'; '__main__' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ac37f9eef0cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpysndfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mformatinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msndfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpysndfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msupported_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupported_endianness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupported_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPyaudioException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPyaudioIOError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__main__.pysndfile'; '__main__' is not a package"
     ]
    }
   ],
   "source": [
    "from pysndfile import formatinfo, sndfile\n",
    "from pysndfile import supported_format, supported_endianness, supported_encoding, PyaudioException, PyaudioIOError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
